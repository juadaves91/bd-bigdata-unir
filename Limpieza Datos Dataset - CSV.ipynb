{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b1944d",
   "metadata": {},
   "source": [
    "# 0. Init pySpark and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ce82fbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/juandavidescobarescobar/Documents/Unir/Materias/BD Big Data/Actividad 1'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import chardet\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "334f2c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing pyspark installation\n",
    "import findspark\n",
    "findspark.init('/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2')\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "45a75350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate Spark Context\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('SparkApp').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e580113",
   "metadata": {},
   "source": [
    "# 1. Funciones de validacion - CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "En esta parte del código se encarga de validar la lectura correcta del archivo en formato CSV, de acuerdo \n",
    "a sus propiedades (encabezados, encoding, separador de linea, separador de columna, filas, columnas) y el \n",
    "esquema o tipología de los datos.\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b00ca5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Descripción: Retorna boolean que determina si el archivo cuenta con el encoding UTF-8.\n",
    "Responsables: Juan David Escobar E\n",
    "Fecha: 30/11/2021\n",
    "\"\"\"\n",
    "\n",
    "def is_valid_encoding_csv(ar_file):\n",
    "    this_encoding = 'UTF-8'\n",
    "    result = chardet.detect(open(ar_file, 'rb').read())\n",
    "    charenc = result['encoding']\n",
    "    return  True if this_encoding in charenc.upper() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ce1f8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Descripción: Retorna los registros duplicados a partir de un Dataframe, y los registros unicos\n",
    "Parámetros:\n",
    "    ar_file -- Archivo a validar\n",
    "    gb_records -- String el cual contiene los nombres de la columna que son unicos del Dataframe.\n",
    "Responsables: Juan David Escobar E\n",
    "Fecha: 01/12/2021\n",
    "\"\"\"\n",
    "\n",
    "def get_duplicates(df_csv, df_pk):\n",
    "    is_error = False\n",
    "    msg_error = ''\n",
    "    separator = ''\n",
    "   \n",
    "    try:      \n",
    "        df_Campo = df_csv.groupby(df_pk).count()\n",
    "        df_duplicados = df_Campo.select(col(df_pk), col(\"count\")).filter(col(\"count\") > 1).collect()\n",
    "        duplicados = [str(df_pk + \": \" + row[df_pk] + \" - Cantidad: \" \\\n",
    "                   + str(row['count'])) for row in df_duplicados]\n",
    "\n",
    "        for i in range(len(duplicados)):\n",
    "            lista_duplicados = duplicados[i].split(\",\")\n",
    "            msg_error += separator + \"[\"+(lista_duplicados[0].replace('\"',''))+\"]\"\n",
    "            separator = ', '\n",
    "        if len(duplicados) == 0:\n",
    "            is_error = True\n",
    "            msg_error = ''\n",
    "            result = (is_error, msg_error)\n",
    "    except Exception as error:\n",
    "        is_error = False\n",
    "        msg_error = 'No se pudo validar duplicados. !ERROR¡: ' + str(error)\n",
    "        result = {'is_error' : is_error, 'msg_error' : msg_error}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66fe65",
   "metadata": {},
   "source": [
    "# 2. Lectura y limpieza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "731080b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Descripción: Lectura desde una ruta local un archivo en formato CSV, el cual se  intenta interpreta \n",
    "             interpretar por primera vez, asumiendo que el archivo posee un encoding tipo UTF-8, no \n",
    "             se especifica esquema, delimitador el caracter \";\", salto de linea el caracter CRLF y \n",
    "             la primera fila con encabezado.\n",
    "Responsables: Juan David Escobar E\n",
    "Fecha: 30/11/2021\n",
    "'''\n",
    "\n",
    "def read_csv():\n",
    "\n",
    "    # File location (https://www.youtube.com/watch?v=-tZbkgTnGs4)\n",
    "    file_location = '/Users/juandavidescobarescobar/Documents/Unir/Materias/BD Big Data/Actividad 1/data_act_03.csv'\n",
    "    file_type = 'csv'\n",
    "\n",
    "    # CSV options\n",
    "    infer_schema = 'true'\n",
    "    first_row_is_header = 'true'\n",
    "    delimiter = ';'\n",
    "    \n",
    "    # Validate encoding UTF-8\n",
    "    is_valid_encode = is_valid_encoding_csv(file_location)\n",
    "    \n",
    "    if is_valid_encode:\n",
    "                \n",
    "        try:        \n",
    "            # The applied options are for CSV files. For other types, these will ignored.\n",
    "            df = spark.read.format(file_type) \\\n",
    "                           .option('inferSchema', infer_schema) \\\n",
    "                           .option('header', first_row_is_header) \\\n",
    "                           .option('sep', delimiter) \\\n",
    "                           .load(file_location) \n",
    "        except Exception as error:\n",
    "                        \n",
    "            print('Error leyendo el archivo: ' + str(error))\n",
    "                \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a76b0",
   "metadata": {},
   "source": [
    "# 3. Limpieza General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "59b9394d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CrimeId: integer (nullable = true)\n",
      " |-- OriginalCrimeTypeName: string (nullable = true)\n",
      " |-- OffenseDate: timestamp (nullable = true)\n",
      " |-- CallTime: string (nullable = true)\n",
      " |-- CallDateTime: timestamp (nullable = true)\n",
      " |-- Disposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- AgencyId: string (nullable = true)\n",
      " |-- Range: string (nullable = true)\n",
      " |-- AddressType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init clean and file validations\n",
    "\n",
    "# 1. Comparar el esquema inferido por pySpark Dataframe vs Los valores almacenados en el archivo\n",
    "\n",
    "df = read_csv()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "90d4212c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n-- CrimeId: integer (nullable = true), El tipo de dato corresponde al que se infiere en el esquema, al ser un\\n            identificador de registro este campo no debería aceptar valores nulos, por lo cual se debe\\n            corregir en el esquema inferido de manera automatica por Spark Dataframe.\\n          \\n-- OriginalCrimeTypeName: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", \\n                          el cual acepta valores nulos sin ningun inconveniente, a simple vista en el archivo se\\n                          logra identificar algunos patrones de información numerica como por ejemplo \"594\" o \\n                          caracteres sin sentido o en código \"lp\" que no concuerdan con la descripción o proposito del \\n                          campo, se puede concluir que son datos errados que quiza debamos limpiar del dataset, teniendo\\n                          muy presente la previa autorización y validación del analista de negocio a cargo.\\n\\n-- OffenseDate: timestamp (nullable = true),  Formato de la fecha:YYYY-MM-DD T HH:MM:SS, para que este formato\\n                identificado a ojo en el archivo CSV cumpla su estructura, se debe generar en una segunda \\n                lectura del dataset leído o inferido de manera original, a un nuevo dataset donde se especifique \\n                el formato que predomina para la fecha y tipo de dato inferido de manera automatica, por medio de\\n                un constrain en un nuevo schema definido por el usuario. Adicional a esto se puede intentar hacer un \\n                casteo de este formato para todos los valores del dataset e identificar valores con error, aunque si\\n                el esquema que lo inferio de manera automatica tiene un tipo de dato timestamp, esto nos da la tranquilidad \\n                de sasber que los valorers estan correctos para este formato de estampa de tiempo.\\n\\n-- CallTime: string (nullable = true) Formato HH:MM, el tipo de dato se infirio de tipo texto, pero identificamos \\n             un factor que se relaciona con la descripción o nombre del campo, para asegurarnos que todos los valores \\n             cumplan con este formato, es importante validarlo por medio de una expresión regular, excluyendo \\n             aquellos valores nulos. Otra manera es recrear el dataset con un esquema especificado en el cual\\n             asignemos este tipo de dato como timestamp con el formato HH:MM.\\n\\n-- CallDateTime: timestamp (nullable = true), Aplica la misma descripción que se especifico para el campo \\n                 \"OffenseDate\"\\n                 \\n-- Disposition: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", tambien \\n                se analiza la longitud de los caracteres de cada campo, la cual es de 3 y en mayuscula, ejemplo \"REP\".\\n                A simple vista se detectan algunos campos vacios, lo cual es normal, pero se detectan valores que no \\n                cumplen el mismo patron 3 caracteres en letra mayuscula, se identifican valores como por ejemplo:\\n                \"Not recorded\", lo cual se puede asumir que es un Dummy que se almaceno ya que no se disponia el \\n                valor, en este caso lo mejor es limpiar esta información para dejar la información mas consistente.\\n\\n-- Address: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", lo cual es un\\n            buen indicio ya que es el tipo de dato comunmente usado para las direcciones en la mayoría de sistemas\\n            de información, ya que almacenan valores alfanumericos. No se identifican valores atipicos. \\n\\n-- City: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", lo cual es un\\n            buen indicio ya que es el tipo de dato comunmente usado para las ciudades, los valores tienen la descipcion\\n            completa de una sola ciudad la cual es San Francisco, algunos campos tienen valores nulos, los cuales\\n            se deben dejar tal cual ya que no se posee información de cual ciudad del estado CA: California pueden\\n            pertenecer Ej: (Los Angeles, San Francisco, San Jose, entre otros). El factor común es San Francisco, pero\\n            lo correcto sería reportar estos campos a los dueños de la informacón para corregirlos en una carga posterior\\n            y actualizarlos por el ID.\\n\\n-- State: string (nullable = true) CA 2 CHARACTERS, Los valores concuerdan con el tipo de dato inferido \"String\", lo cual\\n          es un buen indicio ya que es el tipo de dato comunmente usado para las estados, \\n          https://es.wikipedia.org/wiki/Anexo:Abreviaciones_de_los_estados_de_Estados_Unidos. Para validar la calidad\\n          de la información se puede hacer un distinc de la información para conocer las diferentes categorias\\n          registradas, y se debe validar que cumplen el formato de 2 caracteres en mayúsculas.\\n\\n-- AgencyId: string (nullable = true) INT, Los valores no concuerdan con el tipo de dato inferido, ya que \\n             debería ser un INT, esta columna no es relevante en esta sabana de datos ya que es solo el\\n             Id numerico de la agencia donde se reporto el crimen, pero que no brinda una descripción, quiza\\n             este dato se deba conservar para poder relacionar esta tabla de registro de crimenes con otro\\n             dataset de agencias.\\n\\n-- Range: string (nullable = true) Los valores no concuerdan con la descriopcion del campo, ya que normalmente\\n          un rango es un tipo de dato entero, pero que tambien puede ser la descripcion de un limite inferior y\\n          superior, en este caso esta columna no tiene valores y no proporciona información asociada al crimen ya que\\n          todos los valores son null.\\n          \\n-- AddressType: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", para validar\\n                la calidad de la información se puede hacer un distinct de las categorias o tipos de dirección\\n                y validar cual de ellas es un dato errado que no pertenece a un tipo de dirección.\\n'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Al realizar la comparación del paso anterior se encuentran las siguientes anotaciones para cada valor:\n",
    "\n",
    "'''\n",
    "-- CrimeId: integer (nullable = true), El tipo de dato corresponde al que se infiere en el esquema, al ser un\n",
    "            identificador de registro este campo no debería aceptar valores nulos, por lo cual se debe\n",
    "            corregir en el esquema inferido de manera automatica por Spark Dataframe.\n",
    "          \n",
    "-- OriginalCrimeTypeName: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", \n",
    "                          el cual acepta valores nulos sin ningun inconveniente, a simple vista en el archivo se\n",
    "                          logra identificar algunos patrones de información numerica como por ejemplo \"594\" o \n",
    "                          caracteres sin sentido o en código \"lp\" que no concuerdan con la descripción o proposito del \n",
    "                          campo, se puede concluir que son datos errados que quiza debamos limpiar del dataset, teniendo\n",
    "                          muy presente la previa autorización y validación del analista de negocio a cargo.\n",
    "\n",
    "-- OffenseDate: timestamp (nullable = true),  Formato de la fecha:YYYY-MM-DD T HH:MM:SS, para que este formato\n",
    "                identificado a ojo en el archivo CSV cumpla su estructura, se debe generar en una segunda \n",
    "                lectura del dataset leído o inferido de manera original, a un nuevo dataset donde se especifique \n",
    "                el formato que predomina para la fecha y tipo de dato inferido de manera automatica, por medio de\n",
    "                un constrain en un nuevo schema definido por el usuario. Adicional a esto se puede intentar hacer un \n",
    "                casteo de este formato para todos los valores del dataset e identificar valores con error, aunque si\n",
    "                el esquema que lo inferio de manera automatica tiene un tipo de dato timestamp, esto nos da la tranquilidad \n",
    "                de sasber que los valorers estan correctos para este formato de estampa de tiempo.\n",
    "\n",
    "-- CallTime: string (nullable = true) Formato HH:MM, el tipo de dato se infirio de tipo texto, pero identificamos \n",
    "             un factor que se relaciona con la descripción o nombre del campo, para asegurarnos que todos los valores \n",
    "             cumplan con este formato, es importante validarlo por medio de una expresión regular, excluyendo \n",
    "             aquellos valores nulos. Otra manera es recrear el dataset con un esquema especificado en el cual\n",
    "             asignemos este tipo de dato como timestamp con el formato HH:MM.\n",
    "\n",
    "-- CallDateTime: timestamp (nullable = true), Aplica la misma descripción que se especifico para el campo \n",
    "                 \"OffenseDate\"\n",
    "                 \n",
    "-- Disposition: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", tambien \n",
    "                se analiza la longitud de los caracteres de cada campo, la cual es de 3 y en mayuscula, ejemplo \"REP\".\n",
    "                A simple vista se detectan algunos campos vacios, lo cual es normal, pero se detectan valores que no \n",
    "                cumplen el mismo patron 3 caracteres en letra mayuscula, se identifican valores como por ejemplo:\n",
    "                \"Not recorded\", lo cual se puede asumir que es un Dummy que se almaceno ya que no se disponia el \n",
    "                valor, en este caso lo mejor es limpiar esta información para dejar la información mas consistente.\n",
    "\n",
    "-- Address: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", lo cual es un\n",
    "            buen indicio ya que es el tipo de dato comunmente usado para las direcciones en la mayoría de sistemas\n",
    "            de información, ya que almacenan valores alfanumericos. No se identifican valores atipicos. \n",
    "\n",
    "-- City: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", lo cual es un\n",
    "            buen indicio ya que es el tipo de dato comunmente usado para las ciudades, los valores tienen la descipcion\n",
    "            completa de una sola ciudad la cual es San Francisco, algunos campos tienen valores nulos, los cuales\n",
    "            se deben dejar tal cual ya que no se posee información de cual ciudad del estado CA: California pueden\n",
    "            pertenecer Ej: (Los Angeles, San Francisco, San Jose, entre otros). El factor común es San Francisco, pero\n",
    "            lo correcto sería reportar estos campos a los dueños de la informacón para corregirlos en una carga posterior\n",
    "            y actualizarlos por el ID.\n",
    "\n",
    "-- State: string (nullable = true) CA 2 CHARACTERS, Los valores concuerdan con el tipo de dato inferido \"String\", lo cual\n",
    "          es un buen indicio ya que es el tipo de dato comunmente usado para las estados, \n",
    "          https://es.wikipedia.org/wiki/Anexo:Abreviaciones_de_los_estados_de_Estados_Unidos. Para validar la calidad\n",
    "          de la información se puede hacer un distinc de la información para conocer las diferentes categorias\n",
    "          registradas, y se debe validar que cumplen el formato de 2 caracteres en mayúsculas.\n",
    "\n",
    "-- AgencyId: string (nullable = true) INT, Los valores no concuerdan con el tipo de dato inferido, ya que \n",
    "             debería ser un INT, esta columna no es relevante en esta sabana de datos ya que es solo el\n",
    "             Id numerico de la agencia donde se reporto el crimen, pero que no brinda una descripción, quiza\n",
    "             este dato se deba conservar para poder relacionar esta tabla de registro de crimenes con otro\n",
    "             dataset de agencias.\n",
    "\n",
    "-- Range: string (nullable = true) Los valores no concuerdan con la descriopcion del campo, ya que normalmente\n",
    "          un rango es un tipo de dato entero, pero que tambien puede ser la descripcion de un limite inferior y\n",
    "          superior, en este caso esta columna no tiene valores y no proporciona información asociada al crimen ya que\n",
    "          todos los valores son null.\n",
    "          \n",
    "-- AddressType: string (nullable = true), Los valores concuerdan con el tipo de dato inferido \"String\", para validar\n",
    "                la calidad de la información se puede hacer un distinct de las categorias o tipos de dirección\n",
    "                y validar cual de ellas es un dato errado que no pertenece a un tipo de dirección.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "36d72904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10051"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Filas leidas\n",
    "\n",
    "df.count()\n",
    "# 10051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0d41faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Columnas leidas\n",
    "\n",
    "len(df.columns)\n",
    "# 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "21ff9abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+-------------------+--------+-------------------+-----------+--------------------+-------------+-----+--------+-----+---------------+\n",
      "|  CrimeId|OriginalCrimeTypeName|        OffenseDate|CallTime|       CallDateTime|Disposition|             Address|         City|State|AgencyId|Range|    AddressType|\n",
      "+---------+---------------------+-------------------+--------+-------------------+-----------+--------------------+-------------+-----+--------+-----+---------------+\n",
      "|160903280|    Assault / Battery|2016-03-30 00:00:00|   18:42|2016-03-30 18:42:00|        REP|100 Block Of Chil...|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160912272|   Homeless Complaint|2016-03-31 00:00:00|   15:31|2016-03-31 15:31:00|        GOA|2300 Block Of Mar...|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160912590|            Susp Info|2016-03-31 00:00:00|   16:49|2016-03-31 16:49:00|        GOA|2300 Block Of Mar...|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160912801|               Report|2016-03-31 00:00:00|   17:38|2016-03-31 17:38:00|        GOA| 500 Block Of 7th St|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160912811|                  594|2016-03-31 00:00:00|   17:42|2016-03-31 17:42:00|        REP|  Beale St/bryant St|San Francisco|   CA|       1| null|   Intersection|\n",
      "|160913003|                Ref'd|2016-03-31 00:00:00|   18:29|2016-03-31 18:29:00|        GOA|     16th St/pond St|San Francisco|   CA|       1| null|   Intersection|\n",
      "|160913050|   Homeless Complaint|2016-03-31 00:00:00|   18:43|2016-03-31 18:43:00|        ADV|Berwick Pl/harris...|San Francisco|   CA|       1| null|   Intersection|\n",
      "|160913056|   Homeless Complaint|2016-03-31 00:00:00|   18:47|2016-03-31 18:47:00|        HAN|Florida St/maripo...|San Francisco|   CA|       1| null|   Intersection|\n",
      "|160913078| Agg Assault / Adw Dv|2016-03-31 00:00:00|   18:52|2016-03-31 18:52:00|         ND|100 Block Of Gene...|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160913103|           Encampment|2016-03-31 00:00:00|   18:57|2016-03-31 18:57:00|        ADV|2700 Block Of Fol...|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160913118|             Burglary|2016-03-31 00:00:00|   18:59|2016-03-31 18:59:00|        REP|400 Block Of Miss...|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160913148|    Suspicious Person|2016-03-31 00:00:00|   19:08|2016-03-31 19:08:00|        GOA|700 Block Of Eddy St|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160913167|                   Ip|2016-03-31 00:00:00|   19:13|2016-03-31 19:13:00|        HAN|1700 Block Of Har...|San Francisco|   CA|       1| null|Common Location|\n",
      "|160913193|   Homeless Complaint|2016-03-31 00:00:00|   19:18|2016-03-31 19:18:00|        GOA|1800 Block Of Mar...|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160913249|   Homeless Complaint|2016-03-31 00:00:00|   19:37|2016-03-31 19:37:00|        ADV| 0 Block Of Elgin Pk|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160913265|   Homeless Complaint|2016-03-31 00:00:00|   19:40|2016-03-31 19:40:00|        HAN|     Vine Tr/pine St|San Francisco|   CA|       1| null|   Intersection|\n",
      "|160913266|           Encampment|2016-03-31 00:00:00|   19:41|2016-03-31 19:41:00|        HAN|1600 Block Of 15t...|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160913285|          Arrest Made|2016-03-31 00:00:00|   19:49|2016-03-31 19:49:00|        ARR|Jones St/golden G...|San Francisco|   CA|       1| null|   Intersection|\n",
      "|160913298|                 Tent|2016-03-31 00:00:00|   19:54|2016-03-31 19:54:00|        ADV|100 Block Of Sout...|San Francisco|   CA|       1| null|Premise Address|\n",
      "|160913309|   Intoxicated Person|2016-03-31 00:00:00|   19:57|2016-03-31 19:57:00|        HAN|300 Block Of Ofar...|San Francisco|   CA|       1| null|Premise Address|\n",
      "+---------+---------------------+-------------------+--------+-------------------+-----------+--------------------+-------------+-----+--------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10051"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Limpieza general - Datos perdidos N/A or None (Elimina las filas duplicadas - por todos los campos)\n",
    "\n",
    "#df.dropna()\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "df.dropna(how='all').show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5bddeea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Limpieza general - Borrar columnas con None / NAN \n",
    "\n",
    "df = df.toPandas().dropna(axis=1, how='all')\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f6512f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrimeId                  10051\n",
       "OriginalCrimeTypeName    10051\n",
       "OffenseDate              10051\n",
       "CallTime                 10051\n",
       "CallDateTime             10051\n",
       "Disposition              10051\n",
       "Address                  10051\n",
       "City                      9730\n",
       "State                    10048\n",
       "AgencyId                 10051\n",
       "AddressType              10051\n",
       "dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Limpieza general - Elimina las filas duplicadas - por todos los campos\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "418b1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Limpieza general - Elimina las filas duplicadas - por \"CrimeId\"\n",
    "\n",
    "df_pk = 'CrimeId'\n",
    "result_duplicates = get_duplicates(df, df_pk)\n",
    "\n",
    "if result_duplicates['is_error']:\n",
    "    df.drop_duplicates(subset = [df_pk])\n",
    "    print('Identificadores duplicados: {0}'.format(result_duplicates['is_error']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51242bab",
   "metadata": {},
   "source": [
    "# 4. Limpieza Específica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d72039f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CrimeId: integer (nullable = false)\n",
      " |-- OriginalCrimeTypeName: string (nullable = true)\n",
      " |-- OffenseDate: timestamp (nullable = true)\n",
      " |-- CallTime: string (nullable = true)\n",
      " |-- CallDateTime: timestamp (nullable = true)\n",
      " |-- Disposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- AgencyId: integer (nullable = true)\n",
      " |-- AddressType: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/01 21:28:44 WARN TaskSetManager: Stage 48 contains a task of very large size (1011 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/12/01 21:28:44 ERROR Executor: Exception in task 0.0 in stage 48.0 (TID 39)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1392, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1405, in verify_default\n",
      "    verify_acceptable_types(obj)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n",
      "    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\n",
      "TypeError: field OffenseDate: TimestampType can not accept object Row() in type <class 'pyspark.sql.types.Row'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/12/01 21:28:44 WARN TaskSetManager: Lost task 0.0 in stage 48.0 (TID 39) (192.168.20.49 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1392, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1405, in verify_default\n",
      "    verify_acceptable_types(obj)\n",
      "  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n",
      "    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\n",
      "TypeError: field OffenseDate: TimestampType can not accept object Row() in type <class 'pyspark.sql.types.Row'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/12/01 21:28:44 ERROR TaskSetManager: Task 0 in stage 48.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o592.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 39) (192.168.20.49 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field OffenseDate: TimestampType can not accept object Row() in type <class 'pyspark.sql.types.Row'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field OffenseDate: TimestampType can not accept object Row() in type <class 'pyspark.sql.types.Row'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p4/kcnrfr9n2rx7nrt7d02f8l880000gn/T/ipykernel_3942/4018646199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o592.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 39) (192.168.20.49 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field OffenseDate: TimestampType can not accept object Row() in type <class 'pyspark.sql.types.Row'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/Users/juandavidescobarescobar/Documents/Apache Spark/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field OffenseDate: TimestampType can not accept object Row() in type <class 'pyspark.sql.types.Row'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# 1. Ajustar esquema de acuerdo a la naturaleza de los datos analizados en el Datset\n",
    "#StructField(\"Range\", StringType(), True),  \\ # col eliminada\n",
    "\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"CrimeId\",IntegerType(),False), \\\n",
    "    StructField(\"OriginalCrimeTypeName\",StringType(),True), \\\n",
    "    StructField(\"OffenseDate\",TimestampType(),True), \\\n",
    "    StructField(\"CallTime\", StringType(), True), \\\n",
    "    StructField(\"CallDateTime\", TimestampType(), True), \\\n",
    "    StructField(\"Disposition\", StringType(), True), \\\n",
    "    StructField(\"Address\", StringType(), True), \\\n",
    "    StructField(\"City\", StringType(), True), \\\n",
    "    StructField(\"State\", StringType(), True), \\\n",
    "    StructField(\"AgencyId\", IntegerType(), True), \\\n",
    "    StructField(\"AddressType\", StringType(), True)  \\\n",
    "  ])\n",
    "\n",
    "data = df.values.tolist()\n",
    "df = spark.createDataFrame(data = data)\n",
    "\n",
    "\n",
    "df_new = spark.createDataFrame(data = df.rdd, schema = schema)\n",
    "df_new.printSchema()\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "493d4664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CrimeId: int, OriginalCrimeTypeName: string, OffenseDate: timestamp, CallTime: string, CallDateTime: timestamp, Disposition: string, Address: string, City: string, State: string, AgencyId: int, AddressType: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Validar el porcentaje de correlación de los datos\n",
    "\n",
    "# df2.stat.corr('CrimeId', 'CallTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Validar la variable media y desviacion estandar para los tipos de datos númericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f085a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b94b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c2ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32010573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5e8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4142c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf8967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "40aa14dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2395780659.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/p4/kcnrfr9n2rx7nrt7d02f8l880000gn/T/ipykernel_3942/2395780659.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    df.limit(20).toPandas().head()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "    #df.show()\n",
    "    df.limit(20).toPandas().head()\n",
    "    df.printSchema()\n",
    "\n",
    "    #len(df.columns)\n",
    "\n",
    "\n",
    "df.limit(20).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "|-- CrimeId: integer (nullable = true)\n",
    "|-- OriginalCrimeTypeName: string (nullable = true)\n",
    "|-- OffenseDate: timestamp (nullable = true)  YYYY-MM-DD T HH:MM:SS\n",
    "|-- CallTime: string (nullable = true) HH:MM\n",
    "|-- CallDateTime: timestamp (nullable = true) YYYY-MM-DD T HH:MM:SS\n",
    "|-- Disposition: string (nullable = true) 3 charascter MAX EX: REP\n",
    "|-- Address: string (nullable = true) \n",
    "|-- City: string (nullable = true)\n",
    "|-- State: string (nullable = true) CA 2 CHARACTERS\n",
    "|-- AgencyId: string (nullable = true) INT (OJO ERROR)\n",
    "|-- Range: string (nullable = true) INT?? OJO SIN DATOS\n",
    "|-- AddressType: string (nullable = true) String\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea354796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe('CrimeId', 'OriginalCrimeTypeName').show()\n",
    "df2.describe('OffenseDate', 'CallTime').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ded53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenar la informacion del DF en una tabla temporal para poder manipularlo mediante consultas\n",
    "\n",
    "temp_table_csv_name = 'crimes'\n",
    "\n",
    "df.createOrReplaceTempView(temp_table_csv_name)\n",
    "df2 = spark.sql('SELECT * FROM ' + temp_table_csv_name)\n",
    "df2.limit(20).toPandas().head()\n",
    "df2.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec26969",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Describe de los valores enteros, para validar los numeros:\n",
    "MAX, MIN, COUNT, MEAN (PROMEDIO) Y LA \n",
    "DESVIACION ESTANDAR\n",
    "\n",
    "X   |X - (~X)         | (X - (~X))^2\n",
    "\n",
    "5   |5 - 15,6 = -10,6 | (-10,6)^2 = 112,36\n",
    "15  |-0,6             | 0,36\n",
    "12  |-3,6             | 12,96\n",
    "18  |2,4              | 5,76\n",
    "28  |12,4             | 153,76\n",
    "                        (285,2)        \n",
    "\n",
    "~X = 5 +15 + 12 +18 + 28 / 5 = 15,6\n",
    "\n",
    "s = raiz(sum( (X - (~X))^2 ) / N-1)\n",
    "s = raiz(285,1 / (5-1)) = 8,44\n",
    "\n",
    "\n",
    "La desviacion estandar me indica la variacion que existe en los datos de la muestra,\n",
    "es decir que tan diferentes o parecidos son.\n",
    "\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([5, 15, 12, 18, 28])\n",
    "y = np.power(x, 2) # Effectively y = x**2\n",
    "\n",
    "plt.errorbar(x, y, linestyle='None', marker='x')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911fb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b460ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------------------------------------------------------------------------------------\n",
    "0. Contar filas\n",
    "0. Contar columnas\n",
    "--------------------------------------------------------------------------------------------\n",
    "Limpieza de datos:\n",
    "\n",
    "1. Datos perdidos N/A or None       \n",
    "\n",
    "Encontrar Nulos\n",
    "df.isnull()\n",
    "\n",
    "Filtrar datos perdidos\n",
    "from numpy import nan as NA\n",
    "df.dropna()\n",
    "\n",
    "Borrar filas que todos los registros sean None / NA\n",
    "df.dropna(how='all')\n",
    "\n",
    "Borrar columnas con None / NA\n",
    "df.dropna(axis=1, how='all')\n",
    "\n",
    "Borrar ciertos NA, es decir solo lo que le indiquemos\n",
    "df.dropna(thresh=2)\n",
    "\n",
    "Rellenar datos con un valor predeterminado\n",
    "df.fillna(0)\n",
    "\n",
    "Rellenar datos con un valor de un key de un dic\n",
    "para la col 1 y 2\n",
    "df.fillna({1:0.5, 2:5})\n",
    "\n",
    "df.fillna(method='ffill') #fordward fill, rellena con el ultimo valor que no era NA\n",
    "df.fillna(method='ffill', limit=1) #lo mismo pero solo para un NAN\n",
    "\n",
    "df.fillna(data.mean()) #rellena con un promedio de los valores de la fila y no con 0\n",
    "\n",
    "--------------------------------------------------------------------------------------------\n",
    "2. Datos duplicados\n",
    "\n",
    "Nos dice las filas que estan duplicadas\n",
    "\n",
    "df.duplicated()\n",
    "\n",
    "Elimina las filas duplicadas\n",
    "\n",
    "df.drop_duplicates()\n",
    "\n",
    "\n",
    "3. Manipulación de strings\n",
    "4. Transformación de datos\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "90e00e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1cbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfcef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
